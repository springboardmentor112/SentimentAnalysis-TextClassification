{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1496bc",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8bc25ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\AAKASH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\AAKASH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\AAKASH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AAKASH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\AAKASH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443aecfc",
   "metadata": {},
   "source": [
    "Reading File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "146a56b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the csv\n",
    "df = pd.read_csv(r'C:\\Users\\AAKASH\\OneDrive\\Documents\\internship\\Emotions_training.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ff7161",
   "metadata": {},
   "source": [
    "Converting into Lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2627e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting text into lower case\n",
    "df['lw_content'] = df['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0ada78",
   "metadata": {},
   "source": [
    "Removing non textual content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5edb533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing non textual content\n",
    "def remove_non_textual(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "df['nt_content'] = df['lw_content'].apply(remove_non_textual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70291da9",
   "metadata": {},
   "source": [
    "Removing Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cdaf2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to strip HTML tags \n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text()\n",
    "df['nonhtml_text'] = df['nt_content'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2ffe71",
   "metadata": {},
   "source": [
    "Removing Extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "333233df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to expand contractions \n",
    "df['ct_data'] = df['nonhtml_text'].apply(lambda x: contractions.fix(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdd16dc",
   "metadata": {},
   "source": [
    "#Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f3ee9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words\n",
    "stopwords.words('english')\n",
    "[punc for punc in string.punctuation]\n",
    "def text_process(msg):\n",
    "  nopunc= [char for char in msg if char not in string.punctuation]\n",
    "  nopunc= ''.join(nopunc)\n",
    "  return ' '.join([word for word in nopunc.split() if word.lower() not in stopwords.words('english')])\n",
    "df['cleaned_data']= df['ct_data'].apply(text_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45adf6de",
   "metadata": {},
   "source": [
    "Lemmatizing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a123e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizing of data\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_sentence(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)  \n",
    "    lemmatized_sentence = ' '.join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens])\n",
    "    return lemmatized_sentence\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()  \n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN) \n",
    "df['lemmatized_text'] = df['cleaned_data'].apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1d092f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                        feel humiliate\n",
       "1     go feel hopeless damn hopeful around someone c...\n",
       "2                    grab minute post feel greedy wrong\n",
       "3     ever feel nostalgic fireplace know still property\n",
       "4                                          feel grouchy\n",
       "                            ...                        \n",
       "95         feel like throw away shitty piece shit paper\n",
       "96    start feel wryly amuse banal comedy error life...\n",
       "97    find every body beautiful want people feel vit...\n",
       "98    hear owner feel victimize association associat...\n",
       "99    say goodbye fam sad cry feel like heartless bi...\n",
       "Name: lemmatized_text, Length: 100, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lemmatized_text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff527438",
   "metadata": {},
   "source": [
    "Convert the Text corpus to a matrix of word counts(tf-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "43949b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Shape of TF-IDF matrix: (16000, 15186)\n"
     ]
    }
   ],
   "source": [
    "def tfidf(data, column):\n",
    "    vectorizer = TfidfVectorizer() \n",
    "    return vectorizer.fit_transform(data[column])\n",
    "\n",
    "tfidf_matrix = tfidf(df, 'text')\n",
    "tfidf_matrix_array = tfidf_matrix.toarray()\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_matrix_array)\n",
    "print(\"Shape of TF-IDF matrix:\", vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379592b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dividing the dataset in to Train (70%), Test (20%) and Validation (10%) datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e6a3b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 11200\n",
      "Test data size: 3200\n",
      "Validation data size: 1600\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(tfidf_matrix_array)\n",
    "train_size = int(0.7 * len(tfidf_matrix_array))\n",
    "test_size = int(0.2 * len(tfidf_matrix_array))\n",
    "val_size = len(tfidf_matrix_array) - train_size - test_size\n",
    "\n",
    "# Split the data\n",
    "train_data = tfidf_matrix_array[:train_size]\n",
    "test_data = tfidf_matrix_array[train_size:train_size+test_size]\n",
    "val_data = tfidf_matrix_array[train_size+test_size:]\n",
    "\n",
    "print(\"Train data size:\", len(train_data))\n",
    "print(\"Test data size:\", len(test_data))\n",
    "print(\"Validation data size:\", len(val_data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
