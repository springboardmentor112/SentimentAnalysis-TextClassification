{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd6fedf5",
   "metadata": {},
   "source": [
    "# Introduction To Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095b42a2",
   "metadata": {},
   "source": [
    "Sentiment analysis refers to analyzing an opinion or feelings about something using data like text or images, regarding almost anything. Sentiment analysis helps companies in their decision-making process. For instance, if public sentiment towards a product is not so good, a company may try to modify the product or stop the production altogether in order to avoid any losses.\n",
    "\n",
    "There are many sources of public sentiment e.g. public interviews, opinion polls, surveys, etc. However, with more and more people joining social media platforms, websites like Facebook and Twitter can be parsed for public sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf47c4d",
   "metadata": {},
   "source": [
    "# Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b6355c",
   "metadata": {},
   "source": [
    "Given reviews about different apps , the task is to predict whether the reviews contains positive, negative, or neutral sentiment about the apps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dcd441",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b31e29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in c:\\users\\prashant\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c84c5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\prashant\\anaconda3\\lib\\site-packages (1.9.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from wordcloud) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from wordcloud) (1.21.5)\n",
      "Requirement already satisfied: pillow in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from wordcloud) (9.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35e429fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting afinn\n",
      "  Downloading afinn-0.1.tar.gz (52 kB)\n",
      "Building wheels for collected packages: afinn\n",
      "  Building wheel for afinn (setup.py): started\n",
      "  Building wheel for afinn (setup.py): finished with status 'done'\n",
      "  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53447 sha256=a7b06fe45aea594ccda179deff96f50a11b27582a042c988efaf2619d577225a\n",
      "  Stored in directory: c:\\users\\prashant\\appdata\\local\\pip\\cache\\wheels\\79\\91\\ee\\8374d9bc8c6c0896a2db75afdfd63d43653902407a0e76cd94\n",
      "Successfully built afinn\n",
      "Installing collected packages: afinn\n",
      "Successfully installed afinn-0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00f8e7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\prashant\\anaconda3\\lib\\site-packages (1.9.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from wordcloud) (9.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from wordcloud) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from wordcloud) (1.21.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.2)\n",
      "\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prashant\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e67556f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Prashant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Prashant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from afinn import Afinn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2f5b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>userName</th>\n",
       "      <th>userImage</th>\n",
       "      <th>content</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "      <th>reviewCreatedVersion</th>\n",
       "      <th>at</th>\n",
       "      <th>replyContent</th>\n",
       "      <th>repliedAt</th>\n",
       "      <th>appVersion</th>\n",
       "      <th>sortOrder</th>\n",
       "      <th>appId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0197c118-5c6f-4a7b-894c-970023d1a350</td>\n",
       "      <td>Mar Zur</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a/ACg8oc...</td>\n",
       "      <td>I have the same recurring tasks to do every da...</td>\n",
       "      <td>11</td>\n",
       "      <td>4.16.6.2</td>\n",
       "      <td>22-07-2020 13:13</td>\n",
       "      <td>Our team will be happy to look into it for you...</td>\n",
       "      <td>23-07-2020 16:32</td>\n",
       "      <td>4.16.6.2</td>\n",
       "      <td>most_relevant</td>\n",
       "      <td>com.anydo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94868fb5-a21d-4ef9-ab85-81b2ed3d0785</td>\n",
       "      <td>Devin Rivera</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a-/ALV-U...</td>\n",
       "      <td>Instead of shopping around, I downloaded Any.d...</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>08-12-2020 06:24</td>\n",
       "      <td>We are not aware of any issues with randomized...</td>\n",
       "      <td>10-12-2020 09:38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>most_relevant</td>\n",
       "      <td>com.anydo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>825da34e-f65d-4ef3-991d-02d5291820d6</td>\n",
       "      <td>Heidi Kinsley</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a/ACg8oc...</td>\n",
       "      <td>Why does every once in a while... out of the b...</td>\n",
       "      <td>6</td>\n",
       "      <td>5.11.1.2</td>\n",
       "      <td>09-07-2021 13:51</td>\n",
       "      <td>Sorry to hear that! It sounds like you might h...</td>\n",
       "      <td>11-07-2021 11:16</td>\n",
       "      <td>5.11.1.2</td>\n",
       "      <td>most_relevant</td>\n",
       "      <td>com.anydo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a49c2875-651a-4c33-b79c-5813780d659e</td>\n",
       "      <td>Daniel Keller</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a/ACg8oc...</td>\n",
       "      <td>Terrible Update! This app used to be perfect f...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-11-2020 01:50</td>\n",
       "      <td>Please note that the tasks in your tasks view ...</td>\n",
       "      <td>17-11-2020 09:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>most_relevant</td>\n",
       "      <td>com.anydo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9482c75e-2e63-46ab-8c94-47273dd6a829</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>This app is deceivingly terrible. There are so...</td>\n",
       "      <td>20</td>\n",
       "      <td>4.14.0.4</td>\n",
       "      <td>31-01-2019 16:19</td>\n",
       "      <td>Hi Ryan, it sounds like you are describing our...</td>\n",
       "      <td>05-02-2019 11:52</td>\n",
       "      <td>4.14.0.4</td>\n",
       "      <td>most_relevant</td>\n",
       "      <td>com.anydo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               reviewId       userName  \\\n",
       "0  0197c118-5c6f-4a7b-894c-970023d1a350        Mar Zur   \n",
       "1  94868fb5-a21d-4ef9-ab85-81b2ed3d0785   Devin Rivera   \n",
       "2  825da34e-f65d-4ef3-991d-02d5291820d6  Heidi Kinsley   \n",
       "3  a49c2875-651a-4c33-b79c-5813780d659e  Daniel Keller   \n",
       "4  9482c75e-2e63-46ab-8c94-47273dd6a829  A Google user   \n",
       "\n",
       "                                           userImage  \\\n",
       "0  https://play-lh.googleusercontent.com/a/ACg8oc...   \n",
       "1  https://play-lh.googleusercontent.com/a-/ALV-U...   \n",
       "2  https://play-lh.googleusercontent.com/a/ACg8oc...   \n",
       "3  https://play-lh.googleusercontent.com/a/ACg8oc...   \n",
       "4  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "\n",
       "                                             content  thumbsUpCount  \\\n",
       "0  I have the same recurring tasks to do every da...             11   \n",
       "1  Instead of shopping around, I downloaded Any.d...              8   \n",
       "2  Why does every once in a while... out of the b...              6   \n",
       "3  Terrible Update! This app used to be perfect f...              5   \n",
       "4  This app is deceivingly terrible. There are so...             20   \n",
       "\n",
       "  reviewCreatedVersion                at  \\\n",
       "0             4.16.6.2  22-07-2020 13:13   \n",
       "1                  NaN  08-12-2020 06:24   \n",
       "2             5.11.1.2  09-07-2021 13:51   \n",
       "3                  NaN  16-11-2020 01:50   \n",
       "4             4.14.0.4  31-01-2019 16:19   \n",
       "\n",
       "                                        replyContent         repliedAt  \\\n",
       "0  Our team will be happy to look into it for you...  23-07-2020 16:32   \n",
       "1  We are not aware of any issues with randomized...  10-12-2020 09:38   \n",
       "2  Sorry to hear that! It sounds like you might h...  11-07-2021 11:16   \n",
       "3  Please note that the tasks in your tasks view ...  17-11-2020 09:31   \n",
       "4  Hi Ryan, it sounds like you are describing our...  05-02-2019 11:52   \n",
       "\n",
       "  appVersion      sortOrder      appId  \n",
       "0   4.16.6.2  most_relevant  com.anydo  \n",
       "1        NaN  most_relevant  com.anydo  \n",
       "2   5.11.1.2  most_relevant  com.anydo  \n",
       "3        NaN  most_relevant  com.anydo  \n",
       "4   4.14.0.4  most_relevant  com.anydo  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770680e2",
   "metadata": {},
   "source": [
    "# Converting the content coloum into lower case format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f596e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"content\"] = df[\"content\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0eebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b1bbe9",
   "metadata": {},
   "source": [
    "# Removing links/urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe9668",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3cad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "for i, row in df.iterrows():\n",
    "    df.at[i, 'content'] = remove_links(row['content'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c5628b",
   "metadata": {},
   "source": [
    "# Removing next line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].str.replace('\\n','')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b19c252",
   "metadata": {},
   "source": [
    "# Removing extra/white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81feaa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(lambda x: ''.join(x.split()))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea57bd4e",
   "metadata": {},
   "source": [
    "# Removing words containing number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d469c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words_with_numbers(text):\n",
    "    return re.sub(r'\\b\\w*\\d\\w*\\b', '', text)\n",
    "\n",
    "# Apply the remove_words_with_numbers function to the content column\n",
    "df['content'] = df['content'].apply(remove_words_with_numbers)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e84357",
   "metadata": {},
   "source": [
    "# Removing special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92345fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "# Apply the function to the content column\n",
    "df['content'] = df['content'].apply(remove_special_characters)\n",
    "print(df['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b953c97",
   "metadata": {},
   "source": [
    "# Removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da77d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\", \".join (stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697e06b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply the function to the content column\n",
    "df['content'] = df['content'].apply(remove_stopwords)\n",
    "print(df['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af52af1",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6952ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bddfe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [porter.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Apply the function to the content column\n",
    "df['content'] = df['content'].apply(stem_text)\n",
    "print(df['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598eea70",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f8b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    # Extract lemmatized tokens\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    # Join the lemmatized tokens into a sentence\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    "print(df['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170d2a2",
   "metadata": {},
   "source": [
    "# Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec10504",
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb6937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    punctuations = string.punctuation\n",
    "    return text.translate(str.maketrans('', '', punctuations))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9101ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bad571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_regex(text):\n",
    "    if isinstance(text, str):\n",
    "        # Replace punctuation with an empty string\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "    else:\n",
    "        # Return the text as is if it's not a string\n",
    "        return text\n",
    "\n",
    "# Apply the function to the Content column\n",
    "df['clean_text'] = df['content'].apply(remove_punctuation_regex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3b987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d119bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('reviews.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206ce30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a2b92e",
   "metadata": {},
   "source": [
    "As we can see their are total of \"Sixteen thousand seven hundred eighty seven\" rows along with \"twelve\" colums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a712ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df.shape)\n",
    "df=df.head(16787)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b2e09",
   "metadata": {},
   "source": [
    "Here we are considering all the total rows present in the dataset for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dcf820",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186691c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991e05ea",
   "metadata": {},
   "source": [
    "Going to sort the index values of content coloum and plotting a bar graph with title \"Count of Reviews by Users\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9953c8a3",
   "metadata": {},
   "source": [
    "# WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "940c80b0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only supported for TrueType fonts",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna())\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create a WordCloud object\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m wordcloud \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmax_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Display the word cloud using matplotlib\u001b[39;00m\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py:642\u001b[0m, in \u001b[0;36mWordCloud.generate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \n\u001b[0;32m    630\u001b[0m \u001b[38;5;124;03m    The input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;124;03m    self\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py:624\u001b[0m, in \u001b[0;36mWordCloud.generate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    608\u001b[0m \n\u001b[0;32m    609\u001b[0m \u001b[38;5;124;03mThe input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;124;03mself\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    623\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_text(text)\n\u001b[1;32m--> 624\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_frequencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py:453\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    451\u001b[0m     font_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 453\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_frequencies\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfrequencies\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mmax_font_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;66;03m# find font sizes\u001b[39;00m\n\u001b[0;32m    456\u001b[0m     sizes \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout_]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py:511\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    508\u001b[0m transposed_font \u001b[38;5;241m=\u001b[39m ImageFont\u001b[38;5;241m.\u001b[39mTransposedFont(\n\u001b[0;32m    509\u001b[0m     font, orientation\u001b[38;5;241m=\u001b[39morientation)\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# get size of resulting text\u001b[39;00m\n\u001b[1;32m--> 511\u001b[0m box_size \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextbbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfont\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransposed_font\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# find possible places using integral image:\u001b[39;00m\n\u001b[0;32m    513\u001b[0m result \u001b[38;5;241m=\u001b[39m occupancy\u001b[38;5;241m.\u001b[39msample_position(box_size[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmargin,\n\u001b[0;32m    514\u001b[0m                                    box_size[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmargin,\n\u001b[0;32m    515\u001b[0m                                    random_state)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\ImageDraw.py:671\u001b[0m, in \u001b[0;36mImageDraw.textbbox\u001b[1;34m(self, xy, text, font, anchor, spacing, align, direction, features, language, stroke_width, embedded_color)\u001b[0m\n\u001b[0;32m    669\u001b[0m     font \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetfont()\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(font, ImageFont\u001b[38;5;241m.\u001b[39mFreeTypeFont):\n\u001b[1;32m--> 671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly supported for TrueType fonts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    672\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGBA\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m embedded_color \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfontmode\n\u001b[0;32m    673\u001b[0m bbox \u001b[38;5;241m=\u001b[39m font\u001b[38;5;241m.\u001b[39mgetbbox(\n\u001b[0;32m    674\u001b[0m     text, mode, direction, features, language, stroke_width, anchor\n\u001b[0;32m    675\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Only supported for TrueType fonts"
     ]
    }
   ],
   "source": [
    "text = ' '.join(df['content'].dropna())\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(width=800, height=400,max_words=200, background_color='white').generate(text)\n",
    "\n",
    "# Display the word cloud using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8a010",
   "metadata": {},
   "source": [
    "# Frequency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c2a35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('reviews.csv')\n",
    "\n",
    "# Tokenize the 'content' column and convert to lowercase\n",
    "words = ' '.join(df['content']).lower()\n",
    "tokens = word_tokenize(words)\n",
    "\n",
    "# Filter out stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# Get the frequency of each word\n",
    "word_freq = Counter(filtered_tokens)\n",
    "\n",
    "# Create a DataFrame from the frequency table\n",
    "freq_table = pd.DataFrame(word_freq.items(), columns=['Word', 'Frequency'])\n",
    "\n",
    "# Sort the DataFrame by frequency in descending order\n",
    "freq_table = freq_table.sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "# Display the frequency table\n",
    "print(freq_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e702533",
   "metadata": {},
   "source": [
    "# Frequency for top 10 Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611183d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('reviews.csv')\n",
    "\n",
    "# Tokenize the 'content' column and convert to lowercase\n",
    "words = ' '.join(df['content']).lower()\n",
    "tokens = word_tokenize(words)\n",
    "\n",
    "# Filter out stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# Get the frequency of each word\n",
    "word_freq = Counter(filtered_tokens)\n",
    "\n",
    "# Get the top 10 most frequent words\n",
    "top_words = word_freq.most_common(10)\n",
    "\n",
    "# Create a DataFrame from the top words\n",
    "top_words_df = pd.DataFrame(top_words, columns=['Word', 'Frequency'])\n",
    "\n",
    "# Plot the top 10 words\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_words_df['Word'], top_words_df['Frequency'])\n",
    "plt.title('Top 10 Most Frequent Words in Reviews')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7c2686",
   "metadata": {},
   "source": [
    "# Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f06b551",
   "metadata": {},
   "source": [
    "# Using VADER Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26f71513",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 content Sentiment\n",
      "0      I have the same recurring tasks to do every da...  Negative\n",
      "1      Instead of shopping around, I downloaded Any.d...  Negative\n",
      "2      Why does every once in a while... out of the b...  Negative\n",
      "3      Terrible Update! This app used to be perfect f...  Positive\n",
      "4      This app is deceivingly terrible. There are so...  Positive\n",
      "...                                                  ...       ...\n",
      "16782                                      Excellent app  Positive\n",
      "16783  I love it. Easy to use. Make my life organize....  Positive\n",
      "16784  I love how I could make plans and check the ap...  Positive\n",
      "16785                           Exactly what I needed!!!   Neutral\n",
      "16786                                        Very good ðŸ‘  Positive\n",
      "\n",
      "[16787 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('reviews.csv')\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get the sentiment score for each review\n",
    "def get_sentiment_score(review):\n",
    "    sentiment = analyzer.polarity_scores(review)\n",
    "    if sentiment['compound'] >= 0.05 :\n",
    "        return \"Positive\"\n",
    "    elif sentiment['compound'] <= - 0.05 :\n",
    "        return \"Negative\"\n",
    "    else :\n",
    "        return \"Neutral\"\n",
    "# Apply the function to the 'content' column to get the sentiment score for each review\n",
    "df['Sentiment'] = df['content'].apply(get_sentiment_score)\n",
    "\n",
    "# Display the DataFrame with the added 'Sentiment' column\n",
    "print(df[['content', 'Sentiment']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da1cd75",
   "metadata": {},
   "source": [
    "# Using AFINN Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f09dc8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  sentiment_score\n",
      "0  I have the same recurring tasks to do every da...             -4.0\n",
      "1  Instead of shopping around, I downloaded Any.d...              2.0\n",
      "2  Why does every once in a while... out of the b...            -10.0\n",
      "3  Terrible Update! This app used to be perfect f...              4.0\n",
      "4  This app is deceivingly terrible. There are so...              4.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from afinn import Afinn\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('reviews.csv')\n",
    "\n",
    "# Initialize the Afinn sentiment analyzer\n",
    "afinn = Afinn()\n",
    "\n",
    "# Function to calculate sentiment score\n",
    "def calculate_sentiment(text):\n",
    "    return afinn.score(text)\n",
    "\n",
    "# Apply the function to the content column and create a new column for sentiment scores\n",
    "df['sentiment_score'] = df['content'].apply(calculate_sentiment)\n",
    "\n",
    "# Display the first few rows of the dataframe with the sentiment scores\n",
    "print(df[['content', 'sentiment_score']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a97d36c",
   "metadata": {},
   "source": [
    "# Rule-Based Lexicon Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "258fdcc8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 content  sentiment_score\n",
      "0      I have the same recurring tasks to do every da...          -0.6792\n",
      "1      Instead of shopping around, I downloaded Any.d...          -0.7558\n",
      "2      Why does every once in a while... out of the b...          -0.8847\n",
      "3      Terrible Update! This app used to be perfect f...           0.7901\n",
      "4      This app is deceivingly terrible. There are so...           0.3204\n",
      "...                                                  ...              ...\n",
      "16782                                      Excellent app           0.5719\n",
      "16783  I love it. Easy to use. Make my life organize....           0.9607\n",
      "16784  I love how I could make plans and check the ap...           0.8451\n",
      "16785                           Exactly what I needed!!!           0.0000\n",
      "16786                                        Very good ðŸ‘           0.4927\n",
      "\n",
      "[16787 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('reviews.csv')\n",
    "\n",
    "# Create a SentimentIntensityAnalyzer object\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to get the sentiment score of a sentence\n",
    "def get_sentiment_score(sentence):\n",
    "    sentiment_score = analyzer.polarity_scores(sentence)\n",
    "    return sentiment_score['compound']\n",
    "\n",
    "# Apply the function to the 'content' column\n",
    "df['sentiment_score'] = df['content'].apply(get_sentiment_score)\n",
    "\n",
    "# Display the DataFrame with the sentiment scores\n",
    "print(df[['content', 'sentiment_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af41cae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
