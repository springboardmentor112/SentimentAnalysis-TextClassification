{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4206fdf0-5725-49e2-b3ad-044c766b359f",
   "metadata": {},
   "source": [
    "# TEXT CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64412dfb-b800-4607-a08d-c13897384357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e12e466-1e42-44a9-b547-b19052347f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>i just had a very brief time in the beanbag an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>i am now turning and i feel pathetic that i am...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>i feel strong and good overall</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>i feel like this was such a rude comment and i...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15999</th>\n",
       "      <td>i know a lot but i feel so stupid because i ca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0                                i didnt feel humiliated      0\n",
       "1      i can go from feeling so hopeless to so damned...      0\n",
       "2       im grabbing a minute to post i feel greedy wrong      3\n",
       "3      i am ever feeling nostalgic about the fireplac...      2\n",
       "4                                   i am feeling grouchy      3\n",
       "...                                                  ...    ...\n",
       "15995  i just had a very brief time in the beanbag an...      0\n",
       "15996  i am now turning and i feel pathetic that i am...      0\n",
       "15997                     i feel strong and good overall      1\n",
       "15998  i feel like this was such a rude comment and i...      3\n",
       "15999  i know a lot but i feel so stupid because i ca...      0\n",
       "\n",
       "[16000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loaded the dataFrame from a CSV file named 'reviews.csv'\n",
    "df=pd.read_csv('Emotions_training.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d543b59f-964d-4d82-93d6-66a83a27c21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handles missing values in the 'text' column\n",
    "df.dropna(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0669d345-7d99-4063-9c78-a465478fe291",
   "metadata": {},
   "source": [
    "# Lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "181dfac3-99dc-4265-bb18-67611bc5ce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the 'text' column to lower case\n",
    "df['text'] = df['text'].str.lower() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f281ee0-10d1-47bf-86b8-a1454ce1bb93",
   "metadata": {},
   "source": [
    "# Remove Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6be18b0-88ad-4f8f-9ab0-ce8ca25f3508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to remove URL links using regular expressions\n",
    "def remove_links(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "#apply the function to 'content' column\n",
    "df['text'] = df['text'].apply(remove_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88cda71-f7a8-4871-a570-ce91415977f3",
   "metadata": {},
   "source": [
    "# Remove next lines(\\n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7183b2-1663-42f2-8aa9-6c7ec23b0254",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to remove newline characters from the 'text' column\n",
    "df['text'] = df['text'].str.replace('\\n', '') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1124dd2-7071-4ac5-8f0a-d1f49c97caea",
   "metadata": {},
   "source": [
    "# Words containing numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ada8a0b7-a4b9-4506-a8eb-5eb50d98fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to remove words containing digits from the 'text' column using apply() method and lambda function\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join(word for word in str(x).split() if not re.match('.*\\d.*', str(word))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf378e0-a7f1-40a3-ab9e-ff1b8256b33b",
   "metadata": {},
   "source": [
    "# Extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3545a643-4230-422d-b0be-7213679d150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to remove extra spaces from the 'text' column using resub() and lambda function\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\s+', ' ', x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36383d7-5f40-4056-a579-c4c3b7df6478",
   "metadata": {},
   "source": [
    "# Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45410a09-2d43-4a98-8f3e-07265421caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to remove special characters using regular expression\n",
    "df['text'] = df['text'].str.replace(r'[^a-z\\s]', '', regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ccb0ee-c901-40ce-a470-005af70d16e1",
   "metadata": {},
   "source": [
    "# Removal of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd1cc7c1-ea9c-4457-8dce-a2b6997a902a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Moulya\n",
      "[nltk_data]     R\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "#stopwords for English language\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#function to remove stop words from text\n",
    "def remove_stopwords(text):\n",
    "    #tokenize the text into words\n",
    "    words = text.split()\n",
    "    #to remove stop words from the list of words\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    #to join the filtered words back into a sentence\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text\n",
    "\n",
    "#apply the function to the 'content' column \n",
    "df['text'] = df['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407edeee-672e-42a9-bf44-7bc4ac98a328",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8eec241c-57a3-42c7-8cc8-e2e672d08348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the PorterStemmer class from nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#initializing porter stemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "#to apply stemming to a given text\n",
    "def apply_stemming(text):\n",
    "    words = text.split()  #tokenize the text into words\n",
    "    stemmed_words = [porter_stemmer.stem(word) for word in words]  #apply stemming to each word\n",
    "    stemmed_text = ' '.join(stemmed_words)  #join the stemmed words back into a sentence\n",
    "    return stemmed_text\n",
    "    \n",
    "#applying function to the 'content' column \n",
    "df['text'] = df['text'].apply(apply_stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7829d3-878b-448a-9ec8-89409b2d4bbc",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ea77854-f921-4bbc-bf3a-dfc357b905bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>didnt feel humili</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go feel hopeless damn hope around someon care ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grab minut post feel greedi wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ever feel nostalg fireplac know still properti</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feel grouchi</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>brief time beanbag said anna feel like beaten</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>turn feel pathet still wait tabl sub teach degre</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>feel strong good overal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>feel like rude comment im glad</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15999</th>\n",
       "      <td>know lot feel stupid portray</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0                                      didnt feel humili      0\n",
       "1      go feel hopeless damn hope around someon care ...      0\n",
       "2                   im grab minut post feel greedi wrong      3\n",
       "3         ever feel nostalg fireplac know still properti      2\n",
       "4                                           feel grouchi      3\n",
       "...                                                  ...    ...\n",
       "15995      brief time beanbag said anna feel like beaten      0\n",
       "15996   turn feel pathet still wait tabl sub teach degre      0\n",
       "15997                            feel strong good overal      1\n",
       "15998                     feel like rude comment im glad      3\n",
       "15999                       know lot feel stupid portray      0\n",
       "\n",
       "[16000 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the WordNetLemmatizer class from nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#initializing the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def apply_lemmatization(text):\n",
    "    words = text.split()  # Tokenize the text into words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]  # Apply lemmatization to each word\n",
    "    lemmatized_text = ' '.join(lemmatized_words)  # Join the lemmatized words back into a sentence\n",
    "    return lemmatized_text\n",
    "    \n",
    "#apply function to the 'content' column \n",
    "df['text'] = df['text'].apply(apply_lemmatization)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12cd138-dcb9-4a22-96fd-8b6785eca4d2",
   "metadata": {},
   "source": [
    "# Featuring Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65846e7a-76ff-4515-abd9-037fdcd3e469",
   "metadata": {},
   "source": [
    "Convert the Text corpus to a matrix of words counts. (Vectorize the Text data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "402a36a2-f0ea-434f-a6d3-7c0a3652b314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "Feature Names:\n",
      "['aa' 'aaaaaaand' 'aaaaand' ... 'zum' 'zumba' 'zz']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#loading dataset\n",
    "df = pd.read_csv('Emotions_training.csv')\n",
    "text_column = 'text'\n",
    "\n",
    "#initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "#fit the vectorizer to the corpus and transform the corpus into a matrix of TF-IDF features\n",
    "X = vectorizer.fit_transform(df[text_column])\n",
    "\n",
    "#TF-IDF matrix\n",
    "tfidf_matrix = X.toarray()\n",
    "\n",
    "#feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "#print\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_matrix)\n",
    "print(\"\\nFeature Names:\")\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d2aa19-c545-45ba-a938-d67a46def90b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
